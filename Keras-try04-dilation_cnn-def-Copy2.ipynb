{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.io import loadmat\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, UpSampling2D, Reshape, Permute, Input, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Conv3D, Dense, Dropout, MaxPooling3D, Flatten, Add\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, CSVLogger\n",
    "from keras import backend\n",
    "from PIL import Image\n",
    "from numba import cuda\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import time\n",
    "train_dir = 'G:\\\\datalist\\\\train_dataset_pix\\\\'\n",
    "test_dir = 'G:\\\\datalist\\\\test_dataset_pix\\\\'\n",
    "train_dir_Salinas = 'G:\\\\datalist\\\\train_dataset_Salinas_pix\\\\'\n",
    "test_dir_Salinas = 'G:\\\\datalist\\\\test_dataset_Salinas_pix\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "photo_piece_size = 7\n",
    "\n",
    "def read_data(data_locate): #读取mat数据\n",
    "    data=loadmat(data_locate)\n",
    "    print(data.keys())\n",
    "    return data\n",
    "\n",
    "def time_trans(time): #将秒转换成时分秒格式\n",
    "    hour = int(time/3600)\n",
    "    minute = int(time%3600 / 60)\n",
    "    second = time%60\n",
    "    return str(hour) +' h '+ str(minute) + ' m ' + str(round(second,3)) + ' s'\n",
    "\n",
    "def make_list(datalen,length): #制作随机坐标列表\n",
    "    init=0\n",
    "    data_pix_list= np.empty([np.square(datalen[0]-length),2], dtype = int)\n",
    "    for con in range(datalen[0]-length):\n",
    "        for row in range(datalen[1]-length):\n",
    "            data_pix_list[init]=[con,row]\n",
    "            init+=1\n",
    "    return data_pix_list\n",
    "\n",
    "def predict_photo(data, lable, length, num_list, shrink = [], augment = False): #随机截取出图片并拼接成数据集\n",
    "    photolable_result = np.ones(num_list.shape[0])\n",
    "    for i in range(num_list.shape[0]):\n",
    "        photolable = lable[num_list[i][0]+3][num_list[i][1]+3] #按行截取图像\n",
    "        photolable_result[i] = photolable\n",
    "        for b in range(length):\n",
    "            photo_line = data[num_list[i][0]+b][num_list[i][1]:num_list[i][1]+length] #按行截取图像\n",
    "            photo_line = np.expand_dims(photo_line,axis=0) #增加维度\n",
    "            if b==0:\n",
    "                photo_pick=photo_line\n",
    "            else:\n",
    "                photo_pick=np.concatenate((photo_pick,photo_line),axis=0) #将截取的行拼接成图像\n",
    "            \n",
    "        photo_pick=np.expand_dims(photo_pick,axis=0) #给图像增加维度\n",
    "        if i==0:\n",
    "            photo_result=photo_pick\n",
    "        else:\n",
    "            photo_result=np.concatenate((photo_result,photo_pick),axis=0)  #将截取的图像拼接成数据集\n",
    "    \n",
    "    photo_result = photo_result.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    photo_result = np.expand_dims(photo_result, axis=4)\n",
    "    \n",
    "    if len(shrink): #压缩波段数目\n",
    "        photo_result=photo_result[:, :, :, shrink]\n",
    "  \n",
    "    return photo_result,photolable_result\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"read data from data file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "        return data['data'], data['lable']\n",
    "\n",
    "class MakeData:\n",
    "    def __init__(self, filenames, piece_size):\n",
    "        self.filenames = filenames\n",
    "        self._indicator = 0\n",
    "        self.batch_num=0\n",
    "        self.data, self.lable = load_data(self.filenames[self.batch_num])\n",
    "        self._num_examples = self.data.shape[0]\n",
    "        self.piece_size = self.data.shape[1]\n",
    "        label_all = loadmat('G:\\datalist\\Indian_pines_gt.mat')\n",
    "        label_all = np.array(label_all['indian_pines_gt'])\n",
    "        label_all = np.array(label_all).flatten()\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(label_all)\n",
    "        print(self.le.classes_)\n",
    "        self.num_classes = len(self.le.classes_)\n",
    "        label_all = None\n",
    "        self.list_num = np.array(range(len(self.filenames)))\n",
    "            \n",
    "    def next_batch(self, batch_size):\n",
    "        while True:\n",
    "            \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "            le = LabelEncoder()\n",
    "            end_indicator = self._indicator + batch_size\n",
    "            if end_indicator > self._num_examples:\n",
    "                if self.batch_num == len(self.filenames):\n",
    "                    self.batch_num = 0\n",
    "                    np.random.shuffle(self.list_num)\n",
    "                self.data, self.lable = load_data(self.filenames[self.list_num[self.batch_num]])\n",
    "                self.batch_num += 1\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "\n",
    "            batch_data = self.data[self._indicator: end_indicator]\n",
    "            batch_label = self.lable[self._indicator: end_indicator]\n",
    "            batch_label = np.array(batch_label).flatten()\n",
    "            batch_label = self.le.transform(batch_label)  \n",
    "            batch_label = to_categorical(batch_label, num_classes = self.num_classes)\n",
    "            self._indicator = end_indicator\n",
    "            yield batch_data, batch_label\n",
    "\n",
    "class MakeData_salinas:\n",
    "    def __init__(self, filenames, piece_size):\n",
    "        self.filenames = filenames\n",
    "        self._indicator = 0\n",
    "        self.batch_num=0\n",
    "        self.data, self.lable = load_data(self.filenames[self.batch_num])\n",
    "        self._num_examples = self.data.shape[0]\n",
    "        self.piece_size = self.data.shape[1]\n",
    "        label_all = loadmat('G:\\datalist\\Salinas_gt.mat')\n",
    "        label_all = np.array(label_all['salinas_gt'])\n",
    "        label_all = np.array(label_all).flatten()\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(label_all)\n",
    "        print(self.le.classes_)\n",
    "        self.num_classes = len(self.le.classes_)\n",
    "        label_all = None\n",
    "        self.list_num = np.array(range(len(self.filenames)))\n",
    "            \n",
    "    def next_batch(self, batch_size):\n",
    "        while True:\n",
    "            \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "            le = LabelEncoder()\n",
    "            end_indicator = self._indicator + batch_size\n",
    "            if end_indicator > self._num_examples:\n",
    "                if self.batch_num == len(self.filenames):\n",
    "                    self.batch_num = 0\n",
    "                    np.random.shuffle(self.list_num)\n",
    "                self.data, self.lable = load_data(self.filenames[self.list_num[self.batch_num]])\n",
    "                self.batch_num += 1\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "\n",
    "            batch_data = self.data[self._indicator: end_indicator]\n",
    "            batch_label = self.lable[self._indicator: end_indicator]\n",
    "            batch_label = np.array(batch_label).flatten()\n",
    "            batch_label = self.le.transform(batch_label)  \n",
    "            batch_label = to_categorical(batch_label, num_classes = self.num_classes)\n",
    "            self._indicator = end_indicator\n",
    "            yield batch_data, batch_label            \n",
    "    \n",
    "def time_trans(time):\n",
    "    hour = int(time/3600)\n",
    "    minute = int(time%3600 / 60)\n",
    "    second = time%60\n",
    "    return str(hour) +' h '+ str(minute) + ' m ' + str(round(second,3)) + ' s'\n",
    "\n",
    "train_filenames = [os.path.join(train_dir, '%03d' % i) for i in range(95)]\n",
    "test_filenames = [os.path.join(test_dir, '%03d' % i) for i in range(95)]\n",
    "\n",
    "train_data = MakeData(train_filenames, photo_piece_size)\n",
    "test_data = MakeData(test_filenames, photo_piece_size)\n",
    "\n",
    "def run_cnn(dilation_rate_list, epoch = 36, cool_down_time = 10, per_epoch = 500):\n",
    "    \n",
    "    ###制作模型\n",
    "    inputs = Input((layers, 7, 7, 1))\n",
    "    conv1 = Conv3D(16, (7, 3, 3), activation='relu', padding='same', data_format = \"channels_last\", \n",
    "                   kernel_initializer='he_normal')(inputs)\n",
    "    b1 = BatchNormalization()(conv1)\n",
    "    conv2 = Conv3D(16, (7, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b1)\n",
    "    b2 = BatchNormalization()(conv2)\n",
    "    conv3 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(b2)\n",
    "    b3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b3)\n",
    "    b4 = BatchNormalization()(conv4)\n",
    "    conv5 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(b4)\n",
    "    b5 = BatchNormalization()(conv5)\n",
    "    add1 = Add()([b3, b5])\n",
    "    skip1 = Conv3D(48, (1, 1, 1), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(add1)\n",
    "    skip1_b = BatchNormalization()(skip1)\n",
    "    conv6 = Conv3D(48, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(add1)\n",
    "    b6 = BatchNormalization()(conv6)\n",
    "    conv7 = Conv3D(48, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b6)\n",
    "    b7 = BatchNormalization()(conv7)\n",
    "    add2 = Add()([skip1_b, b7])\n",
    "    skip2 = Conv3D(32, (1, 1, 1), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(add2)\n",
    "    skip2_b = BatchNormalization()(skip2)\n",
    "    conv8 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(add2)\n",
    "    b8 = BatchNormalization()(conv8)\n",
    "    conv9 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b8)\n",
    "    b9 = BatchNormalization()(conv9)\n",
    "    conv10 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                    dilation_rate=dilation_rate_list)(b9)\n",
    "    b10 = BatchNormalization()(conv10)\n",
    "    add3 = Add()([skip2_b, b10])\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(add3)\n",
    "    conv11 = Conv3D(24, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(pool1)\n",
    "    b11 = BatchNormalization()(conv11)\n",
    "    conv12 = Conv3D(24, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b11)\n",
    "    b12 = BatchNormalization()(conv12)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 1, 1))(b12)\n",
    "    conv13 = Conv3D(16, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(pool2)\n",
    "    b13 = BatchNormalization()(conv13)\n",
    "    conv14 = Conv3D(16, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b13)\n",
    "    b14 = BatchNormalization()(conv14)\n",
    "    drop1 = Dropout(rate=0.4)(b14)\n",
    "    flatten1 = Flatten()(drop1)\n",
    "    full1 = Dense(17, activation='softmax')(flatten1)\n",
    "    model_dilation_cnn = Model(input = inputs, output = full1)    \n",
    "    print(model_dilation_cnn.summary())\n",
    "    \n",
    "    ###运行模型\n",
    "    model_dilation_cnn.compile(loss='categorical_crossentropy', optimizer = Adam(lr = 1.0e-3), metrics=['accuracy']) #设置参数\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', #设置损失缩减函数\n",
    "                                  factor = 0.1,\n",
    "                                  patience = 4,\n",
    "                                  verbose = 1,\n",
    "                                  mode = 'auto',\n",
    "                                  epsilon = 0.0001,\n",
    "                                  cooldown = 6,\n",
    "                                  min_lr = 1.0e-6)\n",
    "    save_model = ModelCheckpoint(filepath = 'G:\\\\datalist\\\\model\\\\model_dilation_cnn_num' + str(dilation_rate_list) + '_{epoch:02d}_{val_loss:.4f}.h5', \n",
    "                                 verbose = 1,\n",
    "                                 period = 2)\n",
    "\n",
    "    csv_logger = CSVLogger(filename = 'training_dilation_cnn.log')\n",
    "    class cool_down_class(keras.callbacks.Callback): #设置cpu冷却函数（休眠）\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            for x in range(cool_down_time, -1, -1):\n",
    "                print(\"\\r\" +  \"冷却倒计时\" + str(x) + \"秒\" + \"  \",end = \"\")\n",
    "                time.sleep(1)\n",
    "            print('\\n', end = \"\")\n",
    "    cool_down = cool_down_class()\n",
    "    train_history = model_dilation_cnn.fit_generator(generator = train_data.next_batch(20),  #运行模型\n",
    "                                        steps_per_epoch = per_epoch, \n",
    "                                        epochs = epoch, \n",
    "                                        verbose = 1, \n",
    "                                        validation_data = test_data.next_batch(20), \n",
    "                                        validation_steps = 50,\n",
    "                                        callbacks = [reduce_lr, save_model, cool_down, csv_logger],\n",
    "                                        max_q_size = 1)    \n",
    "    model_dilation_cnn.save('G:\\\\datalist\\\\model\\\\model_dilation_cnn_num' + str(dilation_rate_list)) #保存模型\n",
    "    \n",
    "    ###模型预测\n",
    "    train_locate='G:\\\\datalist\\\\Indian_pines_corrected.mat'\n",
    "    test_locate='G:\\\\datalist\\\\Indian_pines_gt.mat'\n",
    "    photo_piece_size = 7\n",
    "    pixel_predict_list = np.ones((145-photo_piece_size)*(145-photo_piece_size))\n",
    "    photo_predict_pic = np.ones((145-photo_piece_size)*(145-photo_piece_size))\n",
    "    data_train=read_data(train_locate)\n",
    "    data_lable=read_data(test_locate)\n",
    "    num_list = make_list(data_train['indian_pines_corrected'].shape,7)\n",
    "    start_time = time.perf_counter()\n",
    "    last_time = start_time\n",
    "    total_time = 0\n",
    "    for i in range(145-photo_piece_size):\n",
    "        traind,labled=predict_photo(data_train['indian_pines_corrected'],\n",
    "                                   data_lable['indian_pines_gt'],\n",
    "                                   7,\n",
    "                                   num_list[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)])\n",
    "        pred = model_dilation_cnn.predict(traind)\n",
    "        pred = np.argmax(pred, axis = 1)\n",
    "        pixel_predict_list[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)] = pred\n",
    "        photo_predict_pic[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)] = labled\n",
    "        now_time = time.perf_counter()\n",
    "        elapsed = (now_time - last_time)\n",
    "        elapsed_str = time_trans(elapsed)\n",
    "        total_time = now_time - start_time\n",
    "        total_time_str = time_trans(total_time)\n",
    "        last_time =now_time\n",
    "        print(\"\\r\" +  'step:%d, spend time: %s, total time: %s' % ((i+1), elapsed_str, total_time_str) +  \"                 \",end = \"\")\n",
    "    \n",
    "    ###计算精度\n",
    "    matrix = confusion_matrix(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic))\n",
    "    kappa = cohen_kappa_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic))\n",
    "    matrix_diag = np.diag(matrix)\n",
    "    matrix_raw_sum = np.sum(matrix, axis = 0)\n",
    "    each_acc = np.nan_to_num(matrix_diag/matrix_raw_sum)\n",
    "    aver_acc = np.mean(each_acc)\n",
    "    each_acc = np.around(each_acc, decimals=5)\n",
    "    print(str(dilation_rate_list) + ' kappa+oa+aa:')\n",
    "    print(str(np.around(kappa, decimals=3)))\n",
    "    print(str(np.around(accuracy_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic)), decimals=3)))\n",
    "    print(str(np.around(aver_acc, decimals=3)))\n",
    "    txt_filename = 'G:\\\\datalist\\\\z_kappa_oa_aa.txt'\n",
    "    with open(txt_filename, 'a') as file_object:\n",
    "        file_object.write('\\n' + str(dilation_rate_list) + ' kappa+oa+aa:' + '\\n')\n",
    "        file_object.write(str(np.around(kappa, decimals=3)) + '\\n')\n",
    "        file_object.write(str(np.around(accuracy_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic)), decimals=3)) + '\\n')\n",
    "        file_object.write(str(np.around(aver_acc, decimals=3)) + '\\n')\n",
    "    keras.backend.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "print('done')\n",
    "\n",
    "def run_cnn_salinas(dilation_rate_list, epoch = 36, cool_down_time = 10, per_epoch = 500):\n",
    "    \n",
    "    ###制作模型\n",
    "    inputs = Input((204, 7, 7, 1))\n",
    "    conv1 = Conv3D(16, (7, 3, 3), activation='relu', padding='same', data_format = \"channels_last\", \n",
    "                   kernel_initializer='he_normal')(inputs)\n",
    "    b1 = BatchNormalization()(conv1)\n",
    "    conv2 = Conv3D(16, (7, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(b1)\n",
    "    b2 = BatchNormalization()(conv2)\n",
    "    conv3 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(b2)\n",
    "    b3 = BatchNormalization()(conv3)\n",
    "    conv4 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b3)\n",
    "    b4 = BatchNormalization()(conv4)\n",
    "    conv5 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(b4)\n",
    "    b5 = BatchNormalization()(conv5)\n",
    "    add1 = Add()([b3, b5])\n",
    "    skip1 = Conv3D(48, (1, 1, 1), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(add1)\n",
    "    skip1_b = BatchNormalization()(skip1)\n",
    "    conv6 = Conv3D(48, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(add1)\n",
    "    b6 = BatchNormalization()(conv6)\n",
    "    conv7 = Conv3D(48, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b6)\n",
    "    b7 = BatchNormalization()(conv7)\n",
    "    add2 = Add()([skip1_b, b7])\n",
    "    skip2 = Conv3D(32, (1, 1, 1), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(add2)\n",
    "    skip2_b = BatchNormalization()(skip2)\n",
    "    conv8 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(add2)\n",
    "    b8 = BatchNormalization()(conv8)\n",
    "    conv9 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b8)\n",
    "    b9 = BatchNormalization()(conv9)\n",
    "    conv10 = Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                    dilation_rate=dilation_rate_list)(b9)\n",
    "    b10 = BatchNormalization()(conv10)\n",
    "    add3 = Add()([skip2_b, b10])\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(add3)\n",
    "    conv11 = Conv3D(24, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(pool1)\n",
    "    b11 = BatchNormalization()(conv11)\n",
    "    conv12 = Conv3D(24, (3, 5, 5), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b11)\n",
    "    b12 = BatchNormalization()(conv12)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 1, 1))(b12)\n",
    "    conv13 = Conv3D(16, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal', \n",
    "                   dilation_rate=dilation_rate_list)(pool2)\n",
    "    b13 = BatchNormalization()(conv13)\n",
    "    conv14 = Conv3D(16, (3, 3, 3), activation=\"relu\", padding=\"same\", kernel_initializer='he_normal')(b13)\n",
    "    b14 = BatchNormalization()(conv14)\n",
    "    drop1 = Dropout(rate=0.4)(b14)\n",
    "    flatten1 = Flatten()(drop1)\n",
    "    full1 = Dense(17, activation='softmax')(flatten1)\n",
    "    model_dilation_cnn = Model(input = inputs, output = full1)    \n",
    "    print(model_dilation_cnn.summary())\n",
    "    \n",
    "    ###运行模型\n",
    "    model_dilation_cnn.compile(loss='categorical_crossentropy', optimizer = Adam(lr = 1.0e-3), metrics=['accuracy']) #设置参数\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', #设置损失缩减函数\n",
    "                                  factor = 0.1,\n",
    "                                  patience = 4,\n",
    "                                  verbose = 1,\n",
    "                                  mode = 'auto',\n",
    "                                  epsilon = 0.0001,\n",
    "                                  cooldown = 6,\n",
    "                                  min_lr = 1.0e-6)\n",
    "    save_model = ModelCheckpoint(filepath = 'G:\\\\datalist\\\\model_Salinas\\\\model_dilation_cnn_num' + str(dilation_rate_list) + '_{epoch:02d}_{val_loss:.4f}.h5', \n",
    "                                 verbose = 1,\n",
    "                                 period = 2)\n",
    "\n",
    "    csv_logger = CSVLogger(filename = 'training_dilation_cnn.log')\n",
    "    class cool_down_class(keras.callbacks.Callback): #设置cpu冷却函数（休眠）\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            for x in range(cool_down_time, -1, -1):\n",
    "                print(\"\\r\" +  \"冷却倒计时\" + str(x) + \"秒\" + \"  \",end = \"\")\n",
    "                time.sleep(1)\n",
    "            print('\\n', end = \"\")\n",
    "    cool_down = cool_down_class()\n",
    "    train_history = model_dilation_cnn.fit_generator(generator = train_data.next_batch(20),  #运行模型\n",
    "                                        steps_per_epoch = per_epoch, \n",
    "                                        epochs = epoch, \n",
    "                                        verbose = 1, \n",
    "                                        validation_data = test_data.next_batch(20), \n",
    "                                        validation_steps = 50,\n",
    "                                        callbacks = [reduce_lr, save_model, cool_down, csv_logger],\n",
    "                                        max_q_size = 1)    \n",
    "    model_dilation_cnn.save('G:\\\\datalist\\\\model_Salinas\\\\model_dilation_cnn_num' + str(dilation_rate_list)) #保存模型\n",
    "    \n",
    "    ###模型预测\n",
    "    train_locate='G:\\\\datalist\\\\Salinas_corrected.mat'\n",
    "    test_locate='G:\\\\datalist\\\\Salinas_gt.mat'\n",
    "    photo_piece_size = 7\n",
    "    pixel_predict_list = np.ones((512-photo_piece_size)*(217-photo_piece_size))\n",
    "    photo_predict_pic = np.ones((512-photo_piece_size)*(217-photo_piece_size))\n",
    "    data_train=read_data(train_locate)\n",
    "    data_lable=read_data(test_locate)\n",
    "    num_list = make_list(data_train['salinas_corrected'].shape,7)\n",
    "    start_time = time.perf_counter()\n",
    "    last_time = start_time\n",
    "    total_time = 0\n",
    "    for i in range(512-photo_piece_size):\n",
    "        traind,labled=predict_photo(data_train['salinas_corrected'],\n",
    "                                   data_lable['salinas_gt'],\n",
    "                                   7,\n",
    "                                   num_list[i*(217-photo_piece_size):(i+1)*(217-photo_piece_size)])\n",
    "        pred = model_dilation_cnn.predict(traind)\n",
    "        pred = np.argmax(pred, axis = 1)\n",
    "        pixel_predict_list[i*(217-photo_piece_size):(i+1)*(217-photo_piece_size)] = pred\n",
    "        photo_predict_pic[i*(217-photo_piece_size):(i+1)*(217-photo_piece_size)] = labled\n",
    "\n",
    "        now_time = time.perf_counter()\n",
    "        elapsed = (now_time - last_time)\n",
    "        elapsed_str = time_trans(elapsed)\n",
    "        total_time = now_time - start_time\n",
    "        total_time_str = time_trans(total_time)\n",
    "        last_time =now_time\n",
    "        print(\"\\r\" +  'step:%d, spend time: %s, total time: %s' % ((i+1), elapsed_str, total_time_str) +  \"                 \",end = \"\")\n",
    "\n",
    "    \n",
    "    ###计算精度\n",
    "    matrix = confusion_matrix(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic))\n",
    "    kappa = cohen_kappa_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic))\n",
    "    matrix_diag = np.diag(matrix)\n",
    "    matrix_raw_sum = np.sum(matrix, axis = 0)\n",
    "    each_acc = np.nan_to_num(matrix_diag/matrix_raw_sum)\n",
    "    aver_acc = np.mean(each_acc)\n",
    "    each_acc = np.around(each_acc, decimals=5)\n",
    "    print(str(dilation_rate_list) + ' kappa+oa+aa:')\n",
    "    print(str(np.around(kappa, decimals=3)))\n",
    "    print(str(np.around(accuracy_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic)), decimals=3)))\n",
    "    print(str(np.around(aver_acc, decimals=3)))\n",
    "    txt_filename = 'G:\\\\datalist\\\\z_kappa_oa_aa_salinas.txt'\n",
    "    with open(txt_filename, 'a') as file_object:\n",
    "        file_object.write('\\n' + str(dilation_rate_list) + '\\n') \n",
    "        for aa in range(each_acc.shape[0]):\n",
    "            file_object.write(str(each_acc[aa]*100) + '\\n')\n",
    "        file_object.write('kappa+oa+aa:' + '\\n')\n",
    "        file_object.write(str(np.around(kappa, decimals=5)) + '\\n')\n",
    "        file_object.write(str(np.around(accuracy_score(np.ravel(pixel_predict_list), np.ravel(photo_predict_pic)), decimals=3)) + '\\n')\n",
    "        file_object.write(str(np.around(aver_acc, decimals=5)) + '\\n')\n",
    "    keras.backend.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0121 14:18:13.925748  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0121 14:18:13.946691  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0121 14:18:13.950680  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0121 14:18:14.060386  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0121 14:18:15.596299  3824 deprecation.py:506] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0121 14:18:15.689054  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "W0121 14:18:15.758863  3824 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:191: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=40, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 7, 7, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 200, 7, 7, 16 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 7, 7, 16 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 200, 7, 7, 16 16144       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 7, 7, 16 64          conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 200, 7, 7, 32 13856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 7, 7, 32 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 7, 7, 32 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200, 7, 7, 32 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 200, 7, 7, 48 41520       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 7, 7, 48 192         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 200, 7, 7, 48 1584        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 200, 7, 7, 48 62256       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200, 7, 7, 48 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200, 7, 7, 48 192         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200, 7, 7, 48 0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 200, 7, 7, 32 41504       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 7, 7, 32 128         conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 7, 7, 32 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 200, 7, 7, 32 1568        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200, 7, 7, 32 128         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200, 7, 7, 32 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 100, 3, 3, 32 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 100, 3, 3, 24 57624       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 3, 3, 24 96          conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 100, 3, 3, 24 43224       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 3, 3, 24 96          conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 50, 3, 3, 24) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 50, 3, 3, 16) 28816       max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 50, 3, 3, 16) 19216       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 3, 3, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17)           122417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 563,393\n",
      "Trainable params: 562,433\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 14:18:15.897495  3824 deprecation.py:323] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 180s 360ms/step - loss: 1.4551 - acc: 0.5994 - val_loss: 1.2771 - val_acc: 0.5360\n",
      "冷却倒计时0秒  \n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 1.0891 - acc: 0.6489 - val_loss: 1.9737 - val_acc: 0.4560\n",
      "\n",
      "Epoch 00002: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 2, 2)_02_1.9737.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.9808 - acc: 0.6692 - val_loss: 1.1836 - val_acc: 0.5940\n",
      "冷却倒计时0秒  \n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.8670 - acc: 0.7012 - val_loss: 1.2550 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00004: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 2, 2)_04_1.2550.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.7946 - acc: 0.7153 - val_loss: 2.5527 - val_acc: 0.4650\n",
      "冷却倒计时0秒  \n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.6957 - acc: 0.7468 - val_loss: 5.0084 - val_acc: 0.2660\n",
      "\n",
      "Epoch 00006: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 2, 2)_06_5.0084.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.6448 - acc: 0.7638 - val_loss: 3.2475 - val_acc: 0.3920\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "冷却倒计时0秒  \n",
      "Epoch 8/40\n",
      "137/500 [=======>......................] - ETA: 2:00 - loss: 0.4943 - acc: 0.8237"
     ]
    }
   ],
   "source": [
    "run_cnn((2,2,2),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:191: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=40, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 7, 7, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 200, 7, 7, 16 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 7, 7, 16 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 200, 7, 7, 16 16144       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 7, 7, 16 64          conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 200, 7, 7, 32 13856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 7, 7, 32 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 7, 7, 32 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200, 7, 7, 32 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 200, 7, 7, 48 41520       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 7, 7, 48 192         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 200, 7, 7, 48 1584        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 200, 7, 7, 48 62256       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200, 7, 7, 48 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200, 7, 7, 48 192         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200, 7, 7, 48 0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 200, 7, 7, 32 41504       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 7, 7, 32 128         conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 7, 7, 32 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 200, 7, 7, 32 1568        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200, 7, 7, 32 128         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200, 7, 7, 32 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 100, 3, 3, 32 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 100, 3, 3, 24 57624       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 3, 3, 24 96          conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 100, 3, 3, 24 43224       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 3, 3, 24 96          conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 50, 3, 3, 24) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 50, 3, 3, 16) 10384       max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 50, 3, 3, 16) 6928        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 3, 3, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17)           122417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 531,713\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 176s 351ms/step - loss: 1.4654 - acc: 0.6038 - val_loss: 1.8050 - val_acc: 0.4780\n",
      "冷却倒计时0秒  \n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 1.1196 - acc: 0.6340 - val_loss: 3.5455 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00002: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_02_3.5455.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.9809 - acc: 0.6650 - val_loss: 1.9003 - val_acc: 0.4530\n",
      "冷却倒计时0秒  \n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.9018 - acc: 0.6838 - val_loss: 1.3632 - val_acc: 0.5640\n",
      "\n",
      "Epoch 00004: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_04_1.3632.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.7949 - acc: 0.7164 - val_loss: 3.5917 - val_acc: 0.3610\n",
      "冷却倒计时0秒  \n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.7140 - acc: 0.7449 - val_loss: 1.3297 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00006: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_06_1.3297.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.6505 - acc: 0.7625 - val_loss: 0.8660 - val_acc: 0.6890\n",
      "冷却倒计时0秒  \n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.5699 - acc: 0.7889 - val_loss: 1.0481 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00008: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_08_1.0481.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.5050 - acc: 0.8129 - val_loss: 1.2046 - val_acc: 0.6390\n",
      "冷却倒计时0秒  \n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.4472 - acc: 0.8360 - val_loss: 2.1424 - val_acc: 0.5020\n",
      "\n",
      "Epoch 00010: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_10_2.1424.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.4188 - acc: 0.8422 - val_loss: 0.6470 - val_acc: 0.7560\n",
      "冷却倒计时0秒  \n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.3593 - acc: 0.8665 - val_loss: 1.2180 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00012: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_12_1.2180.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.3332 - acc: 0.8789 - val_loss: 0.7336 - val_acc: 0.7430\n",
      "冷却倒计时0秒  \n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.2949 - acc: 0.8917 - val_loss: 0.4602 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00014: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_14_0.4602.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.2599 - acc: 0.9018 - val_loss: 0.6569 - val_acc: 0.7940\n",
      "冷却倒计时0秒  \n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.2320 - acc: 0.9121 - val_loss: 1.4242 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00016: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_16_1.4242.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.2261 - acc: 0.9160 - val_loss: 1.2620 - val_acc: 0.6760\n",
      "冷却倒计时0秒  \n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.1759 - acc: 0.9334 - val_loss: 0.7226 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00018: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_18_0.7226.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.0725 - acc: 0.9747 - val_loss: 0.3410 - val_acc: 0.9090\n",
      "冷却倒计时0秒  \n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.0380 - acc: 0.9885 - val_loss: 0.4199 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00020: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_20_0.4199.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.0245 - acc: 0.9932 - val_loss: 0.4612 - val_acc: 0.8850\n",
      "冷却倒计时0秒  \n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 169s 339ms/step - loss: 0.0179 - acc: 0.9946 - val_loss: 0.4737 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00022: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_22_0.4737.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.5068 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "冷却倒计时0秒  \n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.3761 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00024: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_24_0.3761.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0090 - acc: 0.9983 - val_loss: 0.4093 - val_acc: 0.9070\n",
      "冷却倒计时0秒  \n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0079 - acc: 0.9978 - val_loss: 0.4538 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00026: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_26_0.4538.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0079 - acc: 0.9983 - val_loss: 0.4303 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "冷却倒计时0秒  \n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.4426 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00028: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_28_0.4426.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0067 - acc: 0.9989 - val_loss: 0.4662 - val_acc: 0.8900\n",
      "冷却倒计时0秒  \n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 169s 338ms/step - loss: 0.0070 - acc: 0.9993 - val_loss: 0.3692 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00030: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_30_0.3692.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.0069 - acc: 0.9986 - val_loss: 0.4485 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "冷却倒计时0秒  \n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.3513 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00032: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_32_0.3513.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0073 - acc: 0.9990 - val_loss: 0.4157 - val_acc: 0.9040\n",
      "冷却倒计时0秒  \n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.4571 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00034: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_34_0.4571.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.3796 - val_acc: 0.9130\n",
      "冷却倒计时0秒  \n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.5347 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00036: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_36_0.5347.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.0065 - acc: 0.9987 - val_loss: 0.3573 - val_acc: 0.9060\n",
      "冷却倒计时0秒  \n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0062 - acc: 0.9991 - val_loss: 0.5743 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00038: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_38_0.5743.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0063 - acc: 0.9990 - val_loss: 0.4394 - val_acc: 0.9010\n",
      "冷却倒计时0秒  \n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 172s 343ms/step - loss: 0.0068 - acc: 0.9986 - val_loss: 0.4251 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00040: saving model to G:\\datalist\\model\\model_dilation_cnn_num(5, 2, 2)_40_0.4251.h5\n",
      "冷却倒计时0秒  \n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:138, spend time: 0 h 0 m 0.793 s, total time: 0 h 1 m 51.834 s                 (5, 2, 2) kappa+oa+aa:\n",
      "0.934\n",
      "0.952\n",
      "0.928\n"
     ]
    }
   ],
   "source": [
    "run_cnn((5,2,2),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0121 08:03:07.774223  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0121 08:03:08.073408  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0121 08:03:08.139232  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0121 08:03:08.374603  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0121 08:03:09.885594  9488 deprecation.py:506] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0121 08:03:09.980340  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "W0121 08:03:10.038186  9488 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:191: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=40, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 7, 7, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 200, 7, 7, 16 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 7, 7, 16 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 200, 7, 7, 16 16144       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 7, 7, 16 64          conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 200, 7, 7, 32 13856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 7, 7, 32 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 7, 7, 32 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200, 7, 7, 32 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 200, 7, 7, 48 41520       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 7, 7, 48 192         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 200, 7, 7, 48 1584        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 200, 7, 7, 48 62256       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200, 7, 7, 48 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200, 7, 7, 48 192         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200, 7, 7, 48 0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 200, 7, 7, 32 41504       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 7, 7, 32 128         conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 7, 7, 32 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 200, 7, 7, 32 1568        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200, 7, 7, 32 128         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200, 7, 7, 32 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 100, 3, 3, 32 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 100, 3, 3, 24 57624       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 3, 3, 24 96          conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 100, 3, 3, 24 43224       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 3, 3, 24 96          conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 50, 3, 3, 24) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 50, 3, 3, 16) 10384       max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 50, 3, 3, 16) 6928        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 3, 3, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17)           122417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 531,713\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 08:03:10.247627  9488 deprecation.py:323] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 192s 384ms/step - loss: 1.4462 - acc: 0.5970 - val_loss: 1.4383 - val_acc: 0.5410\n",
      "冷却倒计时0秒  \n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 171s 343ms/step - loss: 1.0803 - acc: 0.6494 - val_loss: 2.0674 - val_acc: 0.4640\n",
      "\n",
      "Epoch 00002: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_02_2.0674.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 172s 343ms/step - loss: 0.9722 - acc: 0.6696 - val_loss: 1.3449 - val_acc: 0.5240\n",
      "冷却倒计时0秒  \n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.8840 - acc: 0.6903 - val_loss: 1.1536 - val_acc: 0.5730\n",
      "\n",
      "Epoch 00004: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_04_1.1536.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.7995 - acc: 0.7131 - val_loss: 1.3871 - val_acc: 0.5430\n",
      "冷却倒计时0秒  \n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.7096 - acc: 0.7406 - val_loss: 1.2593 - val_acc: 0.5630\n",
      "\n",
      "Epoch 00006: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_06_1.2593.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.6497 - acc: 0.7588 - val_loss: 1.1454 - val_acc: 0.6140\n",
      "冷却倒计时0秒  \n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.5772 - acc: 0.7867 - val_loss: 0.6870 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00008: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_08_0.6870.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.5299 - acc: 0.8063 - val_loss: 0.8497 - val_acc: 0.6510\n",
      "冷却倒计时0秒  \n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.4627 - acc: 0.8298 - val_loss: 1.3043 - val_acc: 0.6010\n",
      "\n",
      "Epoch 00010: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_10_1.3043.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.4186 - acc: 0.8411 - val_loss: 1.5289 - val_acc: 0.5480\n",
      "冷却倒计时0秒  \n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.3736 - acc: 0.8616 - val_loss: 0.6900 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00012: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_12_0.6900.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 183s 366ms/step - loss: 0.1971 - acc: 0.9293 - val_loss: 0.3695 - val_acc: 0.8680\n",
      "冷却倒计时0秒  \n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 189s 378ms/step - loss: 0.1246 - acc: 0.9551 - val_loss: 0.3867 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00014: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_14_0.3867.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 187s 375ms/step - loss: 0.1019 - acc: 0.9632 - val_loss: 0.4181 - val_acc: 0.8800\n",
      "冷却倒计时0秒  \n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 188s 376ms/step - loss: 0.0822 - acc: 0.9700 - val_loss: 0.6100 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00016: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_16_0.6100.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 187s 373ms/step - loss: 0.0623 - acc: 0.9794 - val_loss: 0.4388 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "冷却倒计时0秒  \n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 179s 358ms/step - loss: 0.0404 - acc: 0.9875 - val_loss: 0.3951 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00018: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_18_0.3951.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 173s 347ms/step - loss: 0.0340 - acc: 0.9895 - val_loss: 0.3726 - val_acc: 0.9130\n",
      "冷却倒计时0秒  \n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.0302 - acc: 0.9902 - val_loss: 0.3997 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00020: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_20_0.3997.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 172s 344ms/step - loss: 0.0283 - acc: 0.9923 - val_loss: 0.4435 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "冷却倒计时0秒  \n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0290 - acc: 0.9921 - val_loss: 0.3767 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00022: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_22_0.3767.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0268 - acc: 0.9929 - val_loss: 0.3664 - val_acc: 0.9040\n",
      "冷却倒计时0秒  \n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0255 - acc: 0.9939 - val_loss: 0.4084 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00024: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_24_0.4084.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.0281 - acc: 0.9929 - val_loss: 0.3653 - val_acc: 0.8990\n",
      "冷却倒计时0秒  \n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.0274 - acc: 0.9926 - val_loss: 0.6332 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00026: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_26_0.6332.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 173s 345ms/step - loss: 0.0262 - acc: 0.9930 - val_loss: 0.3403 - val_acc: 0.9000\n",
      "冷却倒计时0秒  \n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0275 - acc: 0.9921 - val_loss: 0.3286 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00028: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_28_0.3286.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0264 - acc: 0.9937 - val_loss: 0.4462 - val_acc: 0.9020\n",
      "冷却倒计时0秒  \n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0260 - acc: 0.9927 - val_loss: 0.4557 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00030: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_30_0.4557.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.0269 - acc: 0.9922 - val_loss: 0.3645 - val_acc: 0.9090\n",
      "冷却倒计时0秒  \n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.0261 - acc: 0.9931 - val_loss: 0.4551 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 00032: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_32_0.4551.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 170s 340ms/step - loss: 0.0249 - acc: 0.9936 - val_loss: 0.4020 - val_acc: 0.8960\n",
      "冷却倒计时0秒  \n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.0261 - acc: 0.9934 - val_loss: 0.3281 - val_acc: 0.9170\n",
      "\n",
      "Epoch 00034: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_34_0.3281.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 170s 339ms/step - loss: 0.0244 - acc: 0.9937 - val_loss: 0.3894 - val_acc: 0.9000\n",
      "冷却倒计时0秒  \n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 172s 343ms/step - loss: 0.0262 - acc: 0.9936 - val_loss: 0.4047 - val_acc: 0.9150\n",
      "\n",
      "Epoch 00036: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_36_0.4047.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0242 - acc: 0.9934 - val_loss: 0.3780 - val_acc: 0.8990\n",
      "冷却倒计时0秒  \n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0248 - acc: 0.9938 - val_loss: 0.4762 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00038: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_38_0.4762.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0257 - acc: 0.9920 - val_loss: 0.4280 - val_acc: 0.9000\n",
      "冷却倒计时0秒  \n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0246 - acc: 0.9933 - val_loss: 0.3650 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00040: saving model to G:\\datalist\\model\\model_dilation_cnn_num(6, 2, 2)_40_0.3650.h5\n",
      "冷却倒计时0秒  \n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:138, spend time: 0 h 0 m 0.812 s, total time: 0 h 1 m 52.621 s                 (6, 2, 2) kappa+oa+aa:\n",
      "0.929\n",
      "0.948\n",
      "0.924\n"
     ]
    }
   ],
   "source": [
    "run_cnn((6,2,2),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:191: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=40, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 7, 7, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 200, 7, 7, 16 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 7, 7, 16 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 200, 7, 7, 16 16144       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 7, 7, 16 64          conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 200, 7, 7, 32 13856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 7, 7, 32 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 7, 7, 32 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200, 7, 7, 32 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 200, 7, 7, 48 41520       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 7, 7, 48 192         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 200, 7, 7, 48 1584        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 200, 7, 7, 48 62256       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200, 7, 7, 48 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200, 7, 7, 48 192         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200, 7, 7, 48 0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 200, 7, 7, 32 41504       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 7, 7, 32 128         conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 7, 7, 32 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 200, 7, 7, 32 1568        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200, 7, 7, 32 128         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200, 7, 7, 32 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 100, 3, 3, 32 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 100, 3, 3, 24 57624       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 3, 3, 24 96          conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 100, 3, 3, 24 43224       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 3, 3, 24 96          conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 50, 3, 3, 24) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 50, 3, 3, 16) 10384       max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 50, 3, 3, 16) 6928        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 3, 3, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17)           122417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 531,713\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 177s 354ms/step - loss: 1.4043 - acc: 0.6085 - val_loss: 2.0293 - val_acc: 0.4710\n",
      "冷却倒计时0秒  \n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 1.0809 - acc: 0.6465 - val_loss: 1.7088 - val_acc: 0.5340\n",
      "\n",
      "Epoch 00002: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_02_1.7088.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.9281 - acc: 0.6839 - val_loss: 2.5175 - val_acc: 0.4060\n",
      "冷却倒计时0秒  \n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.8604 - acc: 0.6963 - val_loss: 1.0647 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00004: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_04_1.0647.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.7575 - acc: 0.7259 - val_loss: 1.0325 - val_acc: 0.6240\n",
      "冷却倒计时0秒  \n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.6912 - acc: 0.7484 - val_loss: 1.0529 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00006: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_06_1.0529.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.6281 - acc: 0.7725 - val_loss: 0.7330 - val_acc: 0.7400\n",
      "冷却倒计时0秒  \n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.5590 - acc: 0.7954 - val_loss: 0.8153 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00008: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_08_0.8153.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.5118 - acc: 0.8116 - val_loss: 1.1932 - val_acc: 0.6110\n",
      "冷却倒计时0秒  \n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.4463 - acc: 0.8362 - val_loss: 1.7395 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00010: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_10_1.7395.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.3835 - acc: 0.8580 - val_loss: 0.9168 - val_acc: 0.6680\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "冷却倒计时0秒  \n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.1967 - acc: 0.9292 - val_loss: 0.4512 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00012: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_12_0.4512.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.1146 - acc: 0.9587 - val_loss: 0.4134 - val_acc: 0.8680\n",
      "冷却倒计时0秒  \n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0838 - acc: 0.9716 - val_loss: 0.4310 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00014: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_14_0.4310.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0680 - acc: 0.9750 - val_loss: 0.5086 - val_acc: 0.8720\n",
      "冷却倒计时0秒  \n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0480 - acc: 0.9833 - val_loss: 0.4443 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00016: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_16_0.4443.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 170s 341ms/step - loss: 0.0410 - acc: 0.9861 - val_loss: 0.3985 - val_acc: 0.9010\n",
      "冷却倒计时0秒  \n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0278 - acc: 0.9903 - val_loss: 0.4399 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00018: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_18_0.4399.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0248 - acc: 0.9920 - val_loss: 0.5136 - val_acc: 0.8960\n",
      "冷却倒计时0秒  \n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0261 - acc: 0.9908 - val_loss: 0.6024 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00020: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_20_0.6024.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.4297 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "冷却倒计时0秒  \n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0080 - acc: 0.9981 - val_loss: 0.6341 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00022: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_22_0.6341.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0055 - acc: 0.9991 - val_loss: 0.5030 - val_acc: 0.9020\n",
      "冷却倒计时0秒  \n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.5894 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00024: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_24_0.5894.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.5244 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "冷却倒计时0秒  \n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0034 - acc: 0.9997 - val_loss: 0.4824 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00026: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_26_0.4824.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.4182 - val_acc: 0.9130\n",
      "冷却倒计时0秒  \n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.4864 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00028: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_28_0.4864.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.5335 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "冷却倒计时0秒  \n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.4327 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00030: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_30_0.4327.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.5539 - val_acc: 0.9030\n",
      "冷却倒计时0秒  \n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 171s 341ms/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.4863 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00032: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_32_0.4863.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0036 - acc: 0.9993 - val_loss: 0.5048 - val_acc: 0.8940\n",
      "冷却倒计时0秒  \n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.5003 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00034: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_34_0.5003.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.5198 - val_acc: 0.8990\n",
      "冷却倒计时0秒  \n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 171s 342ms/step - loss: 0.0034 - acc: 0.9995 - val_loss: 0.4575 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00036: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_36_0.4575.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 173s 346ms/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.4761 - val_acc: 0.9100\n",
      "冷却倒计时0秒  \n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 172s 345ms/step - loss: 0.0028 - acc: 0.9999 - val_loss: 0.4689 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00038: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_38_0.4689.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 173s 345ms/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.5969 - val_acc: 0.8970\n",
      "冷却倒计时0秒  \n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 173s 345ms/step - loss: 0.0033 - acc: 0.9995 - val_loss: 0.5884 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00040: saving model to G:\\datalist\\model\\model_dilation_cnn_num(7, 2, 2)_40_0.5884.h5\n",
      "冷却倒计时0秒  \n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:138, spend time: 0 h 0 m 0.786 s, total time: 0 h 1 m 52.696 s                 (7, 2, 2) kappa+oa+aa:\n",
      "0.934\n",
      "0.951\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "run_cnn((7,2,2),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:191: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=500, epochs=40, verbose=1, validation_data=<generator..., validation_steps=50, callbacks=[<keras.ca..., max_queue_size=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 7, 7, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 200, 7, 7, 16 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200, 7, 7, 16 64          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 200, 7, 7, 16 16144       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 7, 7, 16 64          conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 200, 7, 7, 32 13856       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 7, 7, 32 128         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200, 7, 7, 32 128         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 200, 7, 7, 32 27680       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 200, 7, 7, 32 128         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 200, 7, 7, 48 41520       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 7, 7, 48 192         conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 200, 7, 7, 48 1584        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 200, 7, 7, 48 62256       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 200, 7, 7, 48 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200, 7, 7, 48 192         conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200, 7, 7, 48 0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 200, 7, 7, 32 41504       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 200, 7, 7, 32 128         conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 200, 7, 7, 32 128         conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 200, 7, 7, 32 1568        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 200, 7, 7, 32 27680       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 200, 7, 7, 32 128         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200, 7, 7, 32 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 200, 7, 7, 32 0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 100, 3, 3, 32 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 100, 3, 3, 24 57624       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 3, 3, 24 96          conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 100, 3, 3, 24 43224       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100, 3, 3, 24 96          conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 50, 3, 3, 24) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 50, 3, 3, 16) 10384       max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 50, 3, 3, 16) 6928        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 50, 3, 3, 16) 64          conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 3, 3, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7200)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 17)           122417      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 531,713\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "500/500 [==============================] - 371s 742ms/step - loss: 1.4359 - acc: 0.5982 - val_loss: 1.5487 - val_acc: 0.5560\n",
      "冷却倒计时0秒  \n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 364s 728ms/step - loss: 1.0575 - acc: 0.6490 - val_loss: 2.9940 - val_acc: 0.4620\n",
      "\n",
      "Epoch 00002: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_02_2.9940.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 364s 729ms/step - loss: 0.9429 - acc: 0.6721 - val_loss: 1.2320 - val_acc: 0.5660\n",
      "冷却倒计时0秒  \n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 364s 729ms/step - loss: 0.8219 - acc: 0.7077 - val_loss: 0.8909 - val_acc: 0.6610\n",
      "\n",
      "Epoch 00004: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_04_0.8909.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.7606 - acc: 0.7256 - val_loss: 2.2054 - val_acc: 0.3570\n",
      "冷却倒计时0秒  \n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.6596 - acc: 0.7576 - val_loss: 0.8838 - val_acc: 0.6640\n",
      "\n",
      "Epoch 00006: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_06_0.8838.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 365s 731ms/step - loss: 0.5930 - acc: 0.7829 - val_loss: 1.1147 - val_acc: 0.6120\n",
      "冷却倒计时0秒  \n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.5317 - acc: 0.8030 - val_loss: 0.8175 - val_acc: 0.7040\n",
      "\n",
      "Epoch 00008: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_08_0.8175.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.4672 - acc: 0.8253 - val_loss: 1.2816 - val_acc: 0.5940\n",
      "冷却倒计时0秒  \n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.4208 - acc: 0.8491 - val_loss: 1.4259 - val_acc: 0.5550\n",
      "\n",
      "Epoch 00010: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_10_1.4259.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.3731 - acc: 0.8601 - val_loss: 0.5415 - val_acc: 0.8070\n",
      "冷却倒计时0秒  \n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 365s 731ms/step - loss: 0.3337 - acc: 0.8785 - val_loss: 0.6554 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00012: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_12_0.6554.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.2966 - acc: 0.8897 - val_loss: 0.8632 - val_acc: 0.7130\n",
      "冷却倒计时0秒  \n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 365s 731ms/step - loss: 0.2617 - acc: 0.9038 - val_loss: 0.9400 - val_acc: 0.7010\n",
      "\n",
      "Epoch 00014: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_14_0.9400.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.2563 - acc: 0.9032 - val_loss: 0.4045 - val_acc: 0.8510\n",
      "冷却倒计时0秒  \n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 365s 731ms/step - loss: 0.2372 - acc: 0.9142 - val_loss: 0.5027 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00016: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_16_0.5027.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 365s 730ms/step - loss: 0.2109 - acc: 0.9235 - val_loss: 0.5587 - val_acc: 0.7960\n",
      "冷却倒计时0秒  \n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 365s 731ms/step - loss: 0.1702 - acc: 0.9410 - val_loss: 0.4783 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00018: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_18_0.4783.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 367s 735ms/step - loss: 0.1791 - acc: 0.9344 - val_loss: 0.6505 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "冷却倒计时0秒  \n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 367s 735ms/step - loss: 0.0736 - acc: 0.9751 - val_loss: 0.2685 - val_acc: 0.9230\n",
      "\n",
      "Epoch 00020: saving model to G:\\datalist\\model\\model_dilation_cnn_num(2, 6, 6)_20_0.2685.h5\n",
      "冷却倒计时0秒  \n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 367s 734ms/step - loss: 0.0452 - acc: 0.9852 - val_loss: 0.2680 - val_acc: 0.9110\n",
      "冷却倒计时0秒  \n",
      "Epoch 22/40\n",
      " 34/500 [=>............................] - ETA: 5:35 - loss: 0.0246 - acc: 0.9971"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7b9050a2e745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f8617c0accff>\u001b[0m in \u001b[0;36mrun_cnn\u001b[1;34m(dilation_rate_list, epoch, cool_down_time, per_epoch)\u001b[0m\n\u001b[0;32m    189\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                                         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcool_down\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                         max_q_size = 1)    \n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[0mmodel_dilation_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'G:\\\\datalist\\\\model\\\\model_dilation_cnn_num'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilation_rate_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#保存模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_cnn((2,6,6),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cnn((2,7,7),40,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def run_cnn_model(dilation_rate_list):    \n",
    "    model_dilation_cnn = load_model('G:\\\\datalist\\\\model\\\\model_dilation_cnn_num' + str(dilation_rate_list))\n",
    "    train_locate='G:\\\\datalist\\\\Indian_pines_corrected.mat'\n",
    "    test_locate='G:\\\\datalist\\\\Indian_pines_gt.mat'\n",
    "    photo_piece_size = 7\n",
    "    pixel_predict_list = np.ones((145-photo_piece_size)*(145-photo_piece_size))\n",
    "    photo_predict_pic = np.ones((145-photo_piece_size)*(145-photo_piece_size))\n",
    "    data_train=read_data(train_locate)\n",
    "    data_lable=read_data(test_locate)\n",
    "    num_list = make_list(data_train['indian_pines_corrected'].shape,7)\n",
    "    start_time = time.perf_counter()\n",
    "    last_time = start_time\n",
    "    total_time = 0\n",
    "    for i in range(145-photo_piece_size):\n",
    "        traind,labled=predict_photo(data_train['indian_pines_corrected'],\n",
    "                                   data_lable['indian_pines_gt'],\n",
    "                                   7,\n",
    "                                   num_list[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)])\n",
    "        pred = model_dilation_cnn.predict(traind)\n",
    "        pred = np.argmax(pred, axis = 1)\n",
    "        pixel_predict_list[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)] = pred\n",
    "        photo_predict_pic[i*(145-photo_piece_size):(i+1)*(145-photo_piece_size)] = labled\n",
    "        now_time = time.perf_counter()\n",
    "        elapsed = (now_time - last_time)\n",
    "        elapsed_str = time_trans(elapsed)\n",
    "        total_time = now_time - start_time\n",
    "        total_time_str = time_trans(total_time)\n",
    "        last_time =now_time\n",
    "        print(\"\\r\" +  'step:%d, spend time: %s, total time: %s' % ((i+1), elapsed_str, total_time_str) +  \"                 \",end = \"\")\n",
    "    \n",
    "    from PIL import Image\n",
    "    pic_filepath = 'G:\\\\datalist\\\\result_picture\\\\Indian pines_dilation_cnn_' + str(dilation_rate_list) + '.png'\n",
    "\n",
    "    color = np.array([[41 ,36 ,33], [255 ,255 ,0], [0 ,0 ,255], [244 ,164 ,0], [127 ,255 ,212], \n",
    "                      [218 ,112 ,214], [160 ,32 ,240], [30 ,144 ,255], [0 ,255 ,127], [255 ,97 ,0], \n",
    "                      [106 ,90 ,205], [56 ,94 ,15], [64 ,224 ,208], [189 ,252 ,201], [255 ,192 ,203], \n",
    "                      [255 ,235 ,205], [255 ,215 ,0]])\n",
    "\n",
    "    pixel_predict_list = pixel_predict_list.reshape((145-photo_piece_size),(145-photo_piece_size))\n",
    "    pixel_output_pic = np.ones((pixel_predict_list.shape[0], pixel_predict_list.shape[1], 3))\n",
    "    for i in range(pixel_predict_list.shape[0]):\n",
    "        for j in range(pixel_predict_list.shape[1]):\n",
    "            for k in range(3):\n",
    "                #print(pixel_predict_list[i, j])\n",
    "                color_num = int(pixel_predict_list[i, j])\n",
    "                pixel_output_pic[i, j, k] = color[color_num, k]\n",
    "\n",
    "    new_im = Image.fromarray(np.uint8(pixel_output_pic))\n",
    "    new_im.save(pic_filepath)\n",
    "\n",
    "    img = Image.open(pic_filepath)\n",
    "    arrr = np.array(img)\n",
    "    imshow(arrr)\n",
    "    keras.backend.clear_session()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0131 11:38:40.901819  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0131 11:38:40.924757  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0131 11:38:41.016510  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0131 11:38:41.017508  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0131 11:38:42.805236  8532 deprecation.py:506] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0131 11:38:42.838145  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0131 11:38:45.014941  8532 deprecation_wrapper.py:119] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0131 11:38:45.150580  8532 deprecation.py:323] From D:\\Users\\yanmingjing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n",
      "step:138, spend time: 0 h 0 m 0.825 s, total time: 0 h 1 m 56.916 s                 (2, 2, 2) kappa+oa+aa:\n",
      "0.95325\n",
      "0.96566\n",
      "0.94218\n"
     ]
    }
   ],
   "source": [
    "run_cnn_model((2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n",
      "step:138, spend time: 0 h 0 m 0.614 s, total time: 0 h 1 m 25.83 s                  "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_cnn_model((1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_corrected'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'indian_pines_gt'])\n",
      "step:138, spend time: 0 h 0 m 0.845 s, total time: 0 h 1 m 55.968 s                 "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_cnn_model((6,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
